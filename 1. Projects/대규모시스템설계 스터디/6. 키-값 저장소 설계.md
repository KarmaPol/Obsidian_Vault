Key-Value Store은 비관계형 DB
저장되는 모든 값은 고유 식별자를 키로 가져야한다
### 문제 이해 및 설계 범위 확정
- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터 저장 가능
- 높은 가용성, 장애가 있더라도 빠르게 응답
- 높은 규모 확장성, 트래픽 양에 따라 자동 서버 증설, 삭제
- 데이터 일관성 수준은 조정 가능해야함
- 응답 지연시간이 짧아야함
### 단일 서버 키-값 저장소
가장 쉬운 방법은 키-값 쌍 전부를 메모리에 해시테이블로 저장
모든 데이터를 메모리에 넣는게 불가능할수도 있음
#### 개선책
- 데이터 압축
- 자주 쓰는 데이터만 메모리에, 나머지는 디스크에 저장
### 분산 키-값 저장소
#### CAP 정리
- 데이터 일관성 consistency
  분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속해도 언제나 같은 데이터를 봐야한다
- 가용성 availability
  분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받아야한다
- 파티션 감내 partition tolerance
  두 노드 사이 통신 장애(파티션)가 생기더라도 시스템은 계속 동작해야한다
세가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능 하다
#### 실세계 분산 시스템
분산 시스템은 파티션 문제를 피할 수 없다
파티션 문제가 발생하면 **일관성**, **가용성** 중 선택해야 한다
- **일관성을 선택** (은행권 시스템)
  데이터 불일치 문제를 피하기 위해, 다른 서버의 쓰기 연산을 중단
  -> 가용성이 깨진다
- **가용성을 선택**
  낡은 데이터를 반환하더라도 다른 서버의 읽기 연산을 허용
  파티션 문제가 해결되면 새 데이터를 장애 서버에 전송
### 시스템 컴포넌트
키-값 저장소 구현 핵심 컴포넌트 및 기술
#### 데이터 파티션
데이터를 작은 파티션들로 분할한 다음 여러대 서버에 저장
파티션을 나눌때는 다음 문제 고려
- 데이터를 여러 서버에 고르게 분산할 수 있나
- 노드가 추가, 삭제될때 데이터 재배치를 최소화할수 있나
=> 안정 해시가 적합
#### 데이터 다중화
높은 가용성, 안정성을 확보하기 위해 데이터를 N개 서버에 비동기적으로 다중화해야 한다
-> 해시 링 위에 특정 키를 배치
-> 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본 보관
만약 N개의 노드가 실제 물리 서버의 갯수보다 크다면 물리 서버를 중복선택하지 않도록 해야함
#### 데이터 일관성
여러 노드에 다중화된 데이터는 적절히 동기화 필요
##### 정족수 합의 프로토콜
N = 사본 개수
W = 쓰기 연산 정족수, R = 읽기 연산 정족수
W, R이 커지면 일관성 수준이 향상되나 응답속도는 느려진다
- R = 1 , W = N, 빠른 읽기 연산 최적화 시스템
- W = 1, R = N, 빠른 쓰기 연산 최적화 시스템
- W + R > N, 강한 일관성 보장
- W + R <= N, 강한 일관성 보장 X
##### 일관성 모델
- 강한 일관성
  절대로 낡은 데이터 제공 X
- 약한 일관성
  가장 최근에 갱신된 결과를 반환하지 못할 수 있음
- 최종 일관성
  약한 일관성의 형태, 갱신 결과가 결국에는 모든 사본에 동기화되는 모델
모든 사본에 현재 쓰기 연산 결과가 반영될때까지 해당 데이터 읽기/쓰기를 금지하면 강한 일관성 달성 가능
##### 일관성 기법. 데이터 버저닝
데이터의 다중화는 가용성을 높이지만 사본 간 일관성이 깨질 가능성이 높아진다
###### 버저닝 
데이터를 변경할 때마다 해당 데이터의 새로운 버전 생성, 기존 버전은 무시
###### 벡터 시계
(서버, 버전)의 순서쌍을 데이터에 매단 것
(Si, vi)의 형태 - 이미 있으면 vi를 증가시킨다, 없으면 새 항목 (Si, 1)을 만든다

D3({Sx, 2}, {Sy, 1}), D4({Sx, 2},{Sz, 1}) 같은 버전 데이터 충돌 시
해당 데이터 읽은 클라이언트가 해소 후 서버에 기록 D5({Sx,3},{Sy,1},{Sz,1})

D({s0, 1}, {{s1, 1}})은 D({s0, 1}, {s1, 2})의 이전 버전
D({s0, 1}, {{s1, 1}})D({s0, 2}, {s1, 1})-> 충돌 발생
- 벡터시계의 단점
  1. 충돌 감지 및 해소 로직이 클라이언트에 들어가야함 -> 구현 복잡
  2. 서버:버전의 순서쌍 갯수가 빨리 늘어남 -> 임계치를 설정해 오래된 순서쌍을 제거해야한다
     하지만 오래된 순서쌍을 제거하면 버전간 선후 관계가 정확하지 않아 충돌 해소 효율 감소
     => 아마존 실제 서비스에서 문제 없었음
### 장애 처리
장애는 아주 흔하다 -> 장애 감지 기법, 장애 해소 전략 필요
#### 장애 감지
두 대 이상의 서버가 장애를 보고해야 실제 장애가 발생했다고 간주
모든 노드를 연결하는 멀티 캐스팅 채널을 구축하는 방법이 쉽다 -> 하지만 서버가 많으면 비효율적
##### 가십 프로토콜
서버가 많을때는 가십 프로토콜같은 **분산형 장애 감지 솔루션**을 채택하는 편이 효율적
- 각 노드는 멤버십 목록을 유지, 각 멤버 ID와 heartbeat counter을 기록
- 각 노드는 주기적으로 박동 카운터를 증가
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 박동 카운터 목록을 보낸다
- 박동 카운터 목록을 받은 노드는 멤버십 목록을 갱신
- 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버를 장애 상태로 간주
#### 일시적 장애 처리
장애를 감지한 시스템을 가용성을 보장하기 위해 필요한 조치를 취해야 한다
장애 상태인 서버로 가능 요청은 다른 서버가 잠시 맡아 처리
-> 해당 서버가 복구되었을때 일괄 반영하여 데이터 일관성 보존
=> 이를 위해서 임시로 쓰기 연산을 처리한 서버에 단서(hint)를 남겨둔다
-> **단서 후 임시 위탁 기법**
#### 영구 장애 처리
##### 반 엔트로피 프로토콜
사본들을 비교하여 최신 버전으로 갱신하는 과정
- 머클 트리 생성 과정
  1. 키 공간을 버킷으로 분할
  2. 버킷에 저장된 키에 균등 분포 해시 함수를 적용해 해시 값을 계산
  3. 버킷 별로 해시값을 계산해 해당 해시 값을 레이블로 갖는 노드를 생성
  4. 자식 노드로부터 새로운 해시 값을 계산하여, 이진 트리를 상향식으로 구성
루트 노드의 해시값을 비교하는 것으로 시작해 아래쪽으로 비교
-> 모든 데이터를 동기화할 필요 없이 실제로 존재하는 차이의 크기에 비례
### 데이터 센터 장애 처리
정전, 네트워크 장애, 자연 재해 등으로 인해 데이터 센터 장애 발생 가능
-> 여러 데이터 센터에 다중화
#### 시스템 아키텍처 다이어그램
키-값 저장소에 필요한 다양한 기술적 고려사항들을 적용
#### 쓰기 경로
1. 쓰기 요청이 커밋 로그 파일에 기록
2. 데이터가 메모리 캐시에 기록
3. 메모리 캐시 임계치 도달 시 디스크의 SSTable에 기록
#### 읽기 경로
데이터가 메모리에 있다면 반환
데이터가 메모리에 없다면 디스크에서 가져와야함 -> Bloom filter 사용
1. 데이터가 메모리에 있는지 검사
2. 블룸 필터 검사
3. 블룸 필터로 어떤 SSTable에 키가 보관되어 있는지 찾는다
4. 해당 SSTable에서 데이터를 가져온다
5. 해당 데이터를 클라이언트에 반환

[[LSM Tree vs B+ Tree]]