### Standard Probabilistic IR
- P(R|Q,d) = relevance of query with document
### Language Models for IR
- Assumption
  The user has a prototypes a query
- Approach
  view each document as a language model and compute
	- query as LM's parameter
### Language Models i.e. N-gram Models
### N-gram Models
- predict next word w* given history w1, w2, w3...wn-1
#### N-gram: Statical Estimation
- count how often each w1 ~ wn has been used
#### N-gram: MLE
- MLE is general unsuitable for statistical inference in NLP
## Zipf's Law
- Frequency VS. Rank
- 단어 빈도는 지수함수 형태로 나타남 - Pareto 법칙 (20 : 80)
- 한번만 등장한 단어(hapax legomena)가 44%, 두번 등장한 단어가 17%
## Out of Vocabulary(OOV) Words Problem
- It's not practical to calculate n-grams for all words
### Why use P(d|q) for ranking documents?
- 베이즈 정리로 P(q), P(d)는 알고 있으므로 P(q|d)만 계산하면 됨
#### Smoothing 테크닉
- 0인 확률을 곱하면 전체 결과가 0이 되므로, 보정값 설정
## LM: Pros and Cons
- VSM 보다 성능적으로 우수
### Implementation of IR
- Lemur Toolkit
- Apache Lucene
# LLM
- Strong for Word processing
### Theories of LLM
- Sentence Correction 
- Text Completion
- Text Translation